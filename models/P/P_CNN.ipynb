{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee65e63-fa0a-4242-8270-8e889ad49ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.keras import layers\n",
    "import multiprocessing\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8571c3-8001-4e6d-8eca-9c000218bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r map_cut_data_P_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59209aa2-c4d8-47ea-9261-7443cd305980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data_train_P\n",
    "%store -r data_val_P\n",
    "%store -r label_train\n",
    "%store -r label_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b157c579-41d7-467c-a834-984a44196c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "\n",
    "data_train = data_train_P # np.array con formato (#parches,#pix,#pix,#frecuencias)\n",
    "label_train= label_train # np.array con formato (#parches,#pix,#pix,1)\n",
    "\n",
    "data_val = data_val_P # np.array con formato (#parches,#pix,#pix,#frecuencias)\n",
    "label_val= label_val # np.array con formato (#parches,#pix,#pix,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011f48f7-18d4-412f-9e0f-99339ff64001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 21:52:33.101552: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-01 21:52:33.105107: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-01 21:52:33.105184: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-01 21:52:33.105275: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c10a932555b0): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f56110e62c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONFIGURE SESSION\n",
    "\n",
    "# aquí simplemente le estoy pidiendo que use todos los cores de mi ordenador\n",
    "# esto habría que configurarlo para aprovechar los recursos de Altamira\n",
    "\n",
    "ncpu = multiprocessing.cpu_count()\n",
    "config = tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
    "                        intra_op_parallelism_threads=ncpu-1,\n",
    "                        inter_op_parallelism_threads=ncpu-1)\n",
    "tf.compat.v1.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f75750f2-59dd-4dda-b3ac-651bda2e1ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " module_wrapper (ModuleWrapp  (2548, 64, 64, 16)       800       \n",
      " er)                                                             \n",
      "                                                                 \n",
      " group_normalization (GroupN  (2548, 64, 64, 16)       32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " module_wrapper_1 (ModuleWra  (2548, 64, 64, 16)       0         \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_2 (ModuleWra  (2548, 64, 64, 32)       25120     \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " group_normalization_1 (Grou  (2548, 64, 64, 32)       64        \n",
      " pNormalization)                                                 \n",
      "                                                                 \n",
      " module_wrapper_3 (ModuleWra  (2548, 64, 64, 32)       0         \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_4 (ModuleWra  (2548, 32, 32, 32)       0         \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_5 (ModuleWra  (2548, 32, 32, 32)       0         \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_6 (ModuleWra  (2548, 64, 64, 64)       51264     \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " group_normalization_2 (Grou  (2548, 64, 64, 64)       128       \n",
      " pNormalization)                                                 \n",
      "                                                                 \n",
      " module_wrapper_7 (ModuleWra  (2548, 64, 64, 64)       0         \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_8 (ModuleWra  (2548, 64, 64, 1)        65        \n",
      " pper)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,473\n",
      "Trainable params: 77,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DEFINE CNN MODEL\n",
    "#Modelo de David, cambiando la normalización para que sea más estable\n",
    "\n",
    "model=tf.keras.Sequential()\n",
    "\n",
    "# primer bloque convolucional\n",
    "# 16 es el número de filtros que se aplican en esta capa\n",
    "# (7,7) es el tamaño en píxeles de cada filtro\n",
    "# strides indica cada cuántos píxeles aplicar el filtro\n",
    "# con (1,1) se aplica el filtro sobre todos los píxeles de la imagen\n",
    "# con (2, 2) se aplicaría sobre uno sí uno no\n",
    "# el padding indica que hacer cuando aplicas el filtro cerca de los bordes\n",
    "# padding=same considera como cero los píxeles del filtro que caigan fuera de la imagen, manteniendo el tamaño original de la imagen\n",
    "model.add(layers.Conv2D(16, (7, 7), input_shape=(64, 64, 1), \n",
    "                        strides=(1,1), padding='same'))\n",
    "model.add(tfa.layers.GroupNormalization(groups=16))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "# segundo bloque convolucional\n",
    "model.add(layers.Conv2D(32, (7, 7), strides=(1,1),padding='same'))\n",
    "model.add(tfa.layers.GroupNormalization(groups=32))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "# reducción de dimensionalidad (a la salida tienes #pix/2)\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))  \n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "# último bloque de convolución inversa + aumento de dimensionalidad\n",
    "# al pedir strides=(2,2) estás doblando el tamaño de la imagen \n",
    "model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "model.add(tfa.layers.GroupNormalization(groups=64))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "# salida con activación sigmoide para poder interpretar el output como una probabilidad\n",
    "model.add(layers.Conv2D(1, (1, 1), activation='sigmoid'))\n",
    "\n",
    "# escoger función de perdida, métricas de validación y el optimizador\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "model.build(input_shape=(label_train.shape[0], 64, 64, 1)) # he tenido que añadir esta línea para que model.summary() funcione\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb05c67c-4284-420e-8409-bf4ae29965cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2023-10-01 21:57:25.202738\n",
      "Epoch 1/2\n",
      "255/255 [==============================] - 65s 255ms/step - loss: 0.0444 - binary_accuracy: 0.9925 - val_loss: 0.0445 - val_binary_accuracy: 0.9924\n",
      "Epoch 2/2\n",
      "255/255 [==============================] - 63s 248ms/step - loss: 0.0444 - binary_accuracy: 0.9925 - val_loss: 0.0446 - val_binary_accuracy: 0.9924\n",
      "ended at 2023-10-01 21:59:34.033627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, activation_layer_call_fn, activation_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: P_64pix_5epoch_10batch/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: P_64pix_5epoch_10batch/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.044386833906173706, 0.044444069266319275], 'binary_accuracy': [0.9924730062484741, 0.9924730062484741], 'val_loss': [0.04454449936747551, 0.04457428678870201], 'val_binary_accuracy': [0.9924373626708984, 0.9924373626708984]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'P_64pix_5epoch_10batch/history'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING\n",
    "\n",
    "# a la hora de minimizar, en vez trabajar con #parches a la vez, se trabaja\n",
    "# con sub-divisiones de BATCH_SIZE parches\n",
    "BATCH_SIZE=10 # no puede ser un número muy grande para no sobrecargar memoria \n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "# entrenamiento propiamente dicho\n",
    "start_time=datetime.now()\n",
    "print('starting at '+str(start_time))\n",
    "history=model.fit(data_train, label_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "      validation_data=(data_val, label_val))\n",
    "stop_time=datetime.now()\n",
    "print('ended at '+str(stop_time))\n",
    "\n",
    "# el modelo ha de guardase dentro de un directorio llamado \"models\" (cosas de Tensorflow)\n",
    "# esto te guarda únicamente la configuración de la red y los pesos\n",
    "model_name = \"P_64pix_5epoch_10batch\"\n",
    "history_file_name = \"history\"\n",
    "model.save(model_name) # guarda en un folder\n",
    "\n",
    "# para guardar la evolución de la función de perdida y la accuracy durante el entrenamiento \n",
    "# habría que hacer\n",
    "history_dict=history.history\n",
    "with open(history_file_name, 'wb') as fp:\n",
    "    pickle.dump(history_dict, fp, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "print(history_dict)\n",
    "shutil.move(history_file_name, model_name + \"/\" + history_file_name)\n",
    "# esto ya lo puedes guardar en cualquier directorio\n",
    "\n",
    "\n",
    "# puedes cargar cualquier modelo ya entrenado con\n",
    "#model=load_model(\"P_64pix_1epoch_10batch\") \n",
    "# y continuar entrenando, o aplicar la red en modo predicción a los datos que quieras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edd554d8-ea4f-46af-819c-2cede390cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, images, path=None):\n",
    "    print('start predictions')\n",
    "    if path != None:\n",
    "        model = load_model(path+model)\n",
    "    else:\n",
    "        model = load_model(model_path+model)\n",
    "    predictions = model.predict(images, batch_size = BATCH_SIZE, verbose = True)\n",
    "    print('end predictions')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e36ea1-0fef-49f0-8f1a-0e533f98d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predictions\n",
      "319/319 [==============================] - 27s 84ms/step\n",
      "end predictions\n"
     ]
    }
   ],
   "source": [
    "prediction_P_64pix_5epoch_10batch = prediction(model_name, map_cut_data_P_obs, path = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeea5761-7e01-4a06-aa07-3f8d663bb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1epoch = history_dict['loss']\n",
    "val_loss_1epoch = history_dict['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ee6115-949b-4ec0-9fe1-4055a797c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'prediction_P_64pix_1epoch_10batch' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store prediction_P_64pix_1epoch_10batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6af7886-d53e-48cc-b8e0-7982904bdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_3epoch = history_dict['loss']\n",
    "val_loss_3epoch = history_dict['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e20b2790-9519-4d2f-a8fd-89a812b98ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'prediction_P_64pix_3epoch_10batch' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store prediction_P_64pix_3epoch_10batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04196300-d6e9-4fc3-89f5-0a6ff7c3540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_5epoch = history_dict['loss']\n",
    "val_loss_5epoch = history_dict['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc92c2a-af1e-4d15-81b7-570ebd596b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'prediction_P_64pix_5epoch_10batch' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store prediction_P_64pix_5epoch_10batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84423ec6-6a01-42e2-b902-b583a0b07d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_7epoch = history_dict['loss']\n",
    "val_loss_7epoch = history_dict['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a5fef-a73e-4b26-b6b0-f1338c9961ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store prediction_P_64pix_7epoch_10batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f13ec6-37ff-467d-a36e-4e1d8330a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_9epoch = history_dict['loss']\n",
    "val_loss_9epoch = history_dict['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e6a0e-bf55-4279-a7da-213fc0b65713",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store prediction_P_64pix_9epoch_10batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847e686-5dd4-4479-ab85-58a48295271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_1epoch + loss_3epoch + loss_5epoch + loss_7epoch + loss_9epoch\n",
    "val_loss = val_loss_1epoch + val_loss_3epoch + val_loss_5epoch + val_loss_7epoch + val_loss_9epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94179c8b-4e74-4b39-bab7-ceefdc1e97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,10,1), loss, 'b', label='Training Loss')\n",
    "plt.plot(np.arange(1,10,1), val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"Val_loss_9_epochs.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe9af8-9973-4e5c-b172-c6fdc6f013c4",
   "metadata": {},
   "source": [
    "#### **Prediction after adding the foregrounds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcdb9a-eb7a-4dc6-96d3-f013bff5c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r map_cut_data_P_obs_foregrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905279a-36ac-47f6-847a-3aca053a5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, images, path=None):\n",
    "    print('start predictions')\n",
    "    if path != None:\n",
    "        model = load_model(path+model)\n",
    "    else:\n",
    "        model = load_model(model_path+model)\n",
    "    predictions = model.predict(images, batch_size = BATCH_SIZE, verbose = True)\n",
    "    print('end predictions')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5e1d2-dbc8-44a5-a131-794a0099b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_P_64pix_9epoch_10batch_foregrounds = prediction(\"P_64pix_9epoch_10batch\", map_cut_data_P_obs_foregrounds, path = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284eda8-605a-4f80-afb4-611a11029fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store prediction_P_64pix_9epoch_10batch_foregrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a45ee-2beb-45fb-8ee0-4ab6c4cfb528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
